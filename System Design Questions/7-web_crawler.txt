Web Crawler

- Requirements:
    - Functional Requirements:
        - Crawl web pages and index their content.
        - Respect robots.txt and crawling rules.
        - Support fetching content for specific URLs or domains.

    - Non-Functional Requirements:
        - Scalability: Crawl billions of pages efficiently.
        - Politeness: Avoid overwhelming servers.
        - Freshness: Ensure indexed data is up-to-date.

- Core Entities:
    - URL: url_id, url, last_crawled, status.
    - Page Content: content_id, url_id, content, timestamp.

- API:
    - POST /crawl: Initiate a crawl for a URL or domain.
        - Request: { "url": "https://example.com" }
    - GET /content: Fetch indexed content for a URL.
        - Response: { "url": "https://example.com", "content": "HTML content..." }


- High-Level Diagram:
    - URL Frontier: Queue of URLs to crawl.
    - Crawler: Fetches and processes pages.
    - Indexing Engine: Stores and updates page content.
    - Database: Stores URLs, metadata, and content.


- Deep Dive:
    - URL Deduplication: Use Bloom filters to avoid re-crawling.
    - Politeness: Use rate-limiting and respect crawl-delay in robots.txt.
    - Storage:
        - Use a distributed database (e.g., Elasticsearch) for indexing.
        - Store raw HTML in a blob storage system (e.g., S3).
    - Optimization: Prioritize crawling based on URL freshness or importance.